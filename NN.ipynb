{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOemqjOLi3Ni6Oa8zl2qSjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoWeinzettel/projectdatascience/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB9iNJ4a_8RH",
        "outputId": "46071571-afe2-4949-f0bf-411d5e2659eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "78/78 [==============================] - 1s 4ms/step - loss: 814688763904.0000 - val_loss: 1432620928.0000\n",
            "Epoch 2/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 116512936.0000 - val_loss: 206438.7031\n",
            "Epoch 3/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 33541.0781 - val_loss: 201.7906\n",
            "Epoch 4/100\n",
            "78/78 [==============================] - 0s 2ms/step - loss: 62.8831 - val_loss: 54.1397\n",
            "Epoch 5/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.0420 - val_loss: 54.0381\n",
            "Epoch 6/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.2625 - val_loss: 54.7504\n",
            "Epoch 7/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.4267 - val_loss: 54.4444\n",
            "Epoch 8/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.1920 - val_loss: 54.5012\n",
            "Epoch 9/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.7849 - val_loss: 55.5730\n",
            "Epoch 10/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.5602 - val_loss: 54.3144\n",
            "Epoch 11/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.3917 - val_loss: 62.3122\n",
            "Epoch 12/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 54.4356 - val_loss: 56.0714\n",
            "Epoch 13/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.6883 - val_loss: 56.7719\n",
            "Epoch 14/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 52.9004 - val_loss: 54.5874\n",
            "Epoch 15/100\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 53.4023 - val_loss: 58.2476\n",
            "26/26 [==============================] - 0s 1ms/step\n",
            "[[10.954725   8.401498  15.571407 ]\n",
            " [11.579725   8.151498  15.446407 ]\n",
            " [11.079725   7.9952483 15.696407 ]\n",
            " ...\n",
            " [11.142225   7.7764983 15.321407 ]\n",
            " [11.454725   8.276498  15.508907 ]\n",
            " [11.829725   8.182748  15.758907 ]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Vorhersage f端r das Datum 20231017:\n",
            "tavg Vorhersage: 11.579725\n",
            "tmin Vorhersage: 8.073373\n",
            "tmax Vorhersage: 15.727657\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('berlin.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_filtered = df[[\"date\",\"tavg\",\"tmin\",\"tmax\"]]\n",
        "\n",
        "df_filtered= df_filtered.dropna()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X=df_filtered[[\"date\"]].values\n",
        "y=df_filtered[[\"tavg\",\"tmin\",\"tmax\"]].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_dim=1),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(3)])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "#die 10 steht daf端r dass wenn bei 10 aufeinanderfolgenden losses nix verbessert wird stoppt er die durchf端hrungen\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(y_pred)\n",
        "\n",
        "specific_date = np.array([[20240620]])\n",
        "\n",
        "prediction = model.predict(specific_date)\n",
        "\n",
        "print(\"Vorhersage f端r das Datum 20231017:\")\n",
        "print(\"tavg Vorhersage:\", prediction[0, 0])\n",
        "print(\"tmin Vorhersage:\", prediction[0, 1])\n",
        "print(\"tmax Vorhersage:\", prediction[0, 2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}